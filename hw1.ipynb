{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework covers material from the unit on distributed representations. The primary goal is to explore some new techniques for building and assessing VSMs. The code you write as part of the assignment should be useful for research involving vector representations as well.\n",
    "\n",
    "Like all homeworks, this should be submitted via Canvas. All you have to do is paste in your answers (which are all numerical values) and include the SUNetIds of anyone you worked with. Here's a direct link to the homework form:\n",
    "\n",
    "https://canvas.stanford.edu/courses/83399/quizzes/50268\n",
    "\n",
    "__Contents__\n",
    "\n",
    "0. [Questions 1–2: Dice distance [2 points]](#Questions-1–2:-Dice-distance-[2-points])\n",
    "0. [Question 3: t-test reweighting [2 points]](#Question-3:-t-test-reweighting-[2-points])\n",
    "0. [Questions 4–6: Reweighting and co-occurrence frequency [3 points]](#Questions-4–6:-Reweighting-and-co-occurrence-frequency-[3-points])\n",
    "0. [Question 7: Meeting the GloVe objective [1 point]](#Question-7:-Meeting-the-GloVe-objective-[1-point])\n",
    "0. [Question 8: Expressive eloooongation [2 points]](#Question-8:-Expressive-eloooongation-[2-points])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from mittens import GloVe\n",
    "from scipy.stats import pearsonr\n",
    "import vsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 1–2: Dice distance [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implement [Dice distance](https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient) for real-valued vectors of dimension $n$, as\n",
    "\n",
    "$$\\textbf{dice}(u, v) = 1 - \\frac{\n",
    "    2 \\sum_{i=1}^{n}\\min(u_{i}, v_{i})\n",
    "}{\n",
    "    \\sum_{i=1}^{n} u_{i} + v_{i}\n",
    "}$$\n",
    "\n",
    "(You can use `vsm.matching` for part of this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_dist (u,v):\n",
    "    numerator = 0\n",
    "    for i in range(u.shape[0]):\n",
    "        if u[i] > v[i]:\n",
    "            numerator += v[i]\n",
    "        else:\n",
    "            numerator += u[i]\n",
    "    denominator = np.sum(u) + np.sum(v)\n",
    "    return 1 - 2 * numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, you might want to test your implementation. Here's a simple function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dice_implementation(func):\n",
    "    \"\"\"`func` should be an implementation of `dice` as defined above.\"\"\"\n",
    "    X = np.array([\n",
    "        [  4.,   4.,   2.,   0.],\n",
    "        [  4.,  61.,   8.,  18.],\n",
    "        [  2.,   8.,  10.,   0.],\n",
    "        [  0.,  18.,   0.,   5.]]) \n",
    "    assert func(X[0], X[1]).round(5) == 0.80198\n",
    "    assert func(X[1], X[2]).round(5) == 0.67568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dice_implementation(dice_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, use your implementation to measure the distance between A and B and between B and C in the toy `ABC` matrix we used in the first VSM notebook, repeated here for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x     y\n",
       "A   2.0   4.0\n",
       "B  10.0  15.0\n",
       "C  14.0  10.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABC = pd.DataFrame([\n",
    "    [ 2.0,  4.0], \n",
    "    [10.0, 15.0], \n",
    "    [14.0, 10.0]],\n",
    "    index=['A', 'B', 'C'],\n",
    "    columns=['x', 'y']) \n",
    "\n",
    "ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6129032258064516\n",
      "0.18367346938775508\n"
     ]
    }
   ],
   "source": [
    "A = np.array([ 2.0,  4.0]) \n",
    "B = np.array([10.0, 15.0]) \n",
    "C = np.array([14.0, 10.0])\n",
    "print (dice_dist(A, B))\n",
    "print (dice_dist(B, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__To submit:__\n",
    "\n",
    "1. Dice distance between A and B.\n",
    "2. Dice distance between B and C.\n",
    "\n",
    "(The real question, which these values answer, is whether this measure place A and B close together relative to B and C – our goal for that example.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: t-test reweighting [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test statistic can be thought of as a reweighting scheme. For a count matrix $X$, row index $i$, and column index $j$:\n",
    "\n",
    "$$\\textbf{ttest}(X, i, j) = \n",
    "\\frac{\n",
    "    P(X, i, j) - \\big(P(X, i, *)P(X, *, j)\\big)\n",
    "}{\n",
    "\\sqrt{(P(X, i, *)P(X, *, j))}\n",
    "}$$\n",
    "\n",
    "where $P(X, i, j)$ is $X_{ij}$ divided by the total values in $X$, $P(X, i, *)$ is the sum of the values in row $i$ of $X$ divided by the total values in $X$, and $P(X, *, j)$ is the sum of the values in column $j$ of $X$ divided by the total values in $X$.\n",
    "\n",
    "First, implement this reweighting scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(X):\n",
    "    \n",
    "    X = X.values ### Converting pandas back to numpy\n",
    "    \n",
    "    ### Fully Vectorized Approach\n",
    "    \n",
    "    total_x = np.sum(X)\n",
    "    \n",
    "    prod = np.multiply.outer(np.sum(X, axis=1),np.sum(X, axis=0))/total_x**2\n",
    "    \n",
    "    numerator = X/total_x - prod\n",
    "    denominator = np.sqrt(prod)\n",
    "    \n",
    "    result = numerator/denominator\n",
    "    return result\n",
    "\n",
    "### Non-Vectorized Approach ###\n",
    "\n",
    "#     result = np.zeros_like(X)\n",
    "\n",
    "#     for i in range(X.shape[0]):\n",
    "#         for j in range(X.shape[1]):\n",
    "#             result[i,j] = ttest_single(X, i, j, total_x)\n",
    "#     return result\n",
    "    \n",
    "# def ttest_single(X, i, j, total_x):\n",
    "#     prod = np.sum(X[i, :]) * np.sum(X[:,j])\n",
    "#     prod /= total_x ** 2\n",
    "    \n",
    "#     numerator = X[i,j]/total_x - prod\n",
    "#     denominator = np.sqrt(prod)\n",
    "    \n",
    "#     return numerator/denominator\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ttest_implementation(func):\n",
    "    \"\"\"`func` should be an implementation of ttest reweighting as defined above.\"\"\"\n",
    "    X = pd.DataFrame(np.array([\n",
    "        [  4.,   4.,   2.,   0.],\n",
    "        [  4.,  61.,   8.,  18.],\n",
    "        [  2.,   8.,  10.,   0.],\n",
    "        [  0.,  18.,   0.,   5.]]))    \n",
    "    actual = np.array([\n",
    "        [ 0.33056, -0.07689,  0.04321, -0.10532],\n",
    "        [-0.07689,  0.03839, -0.10874,  0.07574],\n",
    "        [ 0.04321, -0.10874,  0.36111, -0.14894],\n",
    "        [-0.10532,  0.07574, -0.14894,  0.05767]])    \n",
    "    predicted = func(X)\n",
    "    assert np.array_equal(predicted.round(5), actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ttest_implementation(ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, apply your implementation to the matrix stored in `imdb_window5-scaled.csv.gz`.\n",
    "\n",
    "__To submit__: the cell value for the row labeled _superb_ and the column labeled _movie_.\n",
    "\n",
    "(The goal here is really to obtain a working implementation of $\\textbf{ttest}$. It could be an ingredient in a winning bake-off entry!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = 'vsmdata'\n",
    "imdb5 = pd.read_csv(\n",
    "    os.path.join(data_home, 'imdb_window5-scaled.csv.gz'), index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171.63315909012354\n",
      "4301\n",
      "2915\n"
     ]
    }
   ],
   "source": [
    "print (imdb5['superb']['movie'])\n",
    "\n",
    "print (imdb5.index.get_loc('superb'))\n",
    "print (imdb5.columns.get_loc('movie'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_array = ttest(imdb5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0008427614547882476\n"
     ]
    }
   ],
   "source": [
    "print (ttest_array[4301][2915])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 4–6: Reweighting and co-occurrence frequency [3 points]\n",
    "\n",
    "We've seen that raw count matrices encode a lot of frequency information. This is not necessarily all bad (stronger words like _superb_ will be rarer than weak ones like _good_ in part because of their more specialized semantics), but we do hope that our reweighting schemes will get us away from these relatively mundane associations. Thus, for any reweighting scheme, we should ask about its correlation with the raw co-occurrence counts.\n",
    "\n",
    "Your task: using [scipy.stats.pearsonr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html), calculate the Pearson correlation coefficient between the raw count values of `imdb5` as loaded in the previous question and the values obtained from applying PMI and Positive PMI to this matrix, and from reweighting each row by its length norm (as defined in the first noteboook for this unit; `vsm.length_norm`). Note: `X.values.ravel()` will give you the vector of values in the `pd.DataFrame` instance `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb5_pmi = vsm.pmi(imdb5, positive = False)\n",
    "imdb5_ppmi = vsm.pmi(imdb5) # default - positive = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5000)\n",
      "(5000, 5000)\n",
      "(25000000,)\n",
      "(25000000,)\n"
     ]
    }
   ],
   "source": [
    "print (imdb5_pmi.shape)\n",
    "print (imdb5.values.shape)\n",
    "\n",
    "imdb5_data = imdb5.values\n",
    "imdb5_pmi = imdb5_pmi.values\n",
    "imdb5_ppmi = imdb5_ppmi.values\n",
    "\n",
    "imdb5_data = imdb5_data.reshape(imdb5.shape[0]**2,)\n",
    "imdb5_pmi = imdb5_pmi.reshape(imdb5.shape[0]**2,)\n",
    "imdb5_ppmi = imdb5_ppmi.reshape(imdb5.shape[0]**2,)\n",
    "\n",
    "print (imdb5_pmi.shape)\n",
    "print (imdb5_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.007653071529112714, 2.2613e-320)\n",
      "(0.04498063308409067, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(pearsonr(imdb5_data, imdb5_pmi))\n",
    "print(pearsonr(imdb5_data, imdb5_ppmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5000)\n",
      "(25000000,)\n"
     ]
    }
   ],
   "source": [
    "imdb5_len_norm = np.apply_along_axis(vsm.length_norm, 1, imdb5.values)\n",
    "print (imdb5_len_norm.shape)\n",
    "\n",
    "imdb5_len_norm = imdb5_len_norm.reshape(imdb5.shape[0]**2,)\n",
    "\n",
    "print (imdb5_len_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.12218662330676429, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(pearsonr(imdb5_data, imdb5_len_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__To submit:__\n",
    "\n",
    "1. Correlation coefficient for the PMI comparison.\n",
    "1. Correlation coefficient for the Positive PMI comparison.\n",
    "1. Correlation coefficient for the length-norm comparison.\n",
    "\n",
    "(The hope is that seeing these values will give you a better sense for how these reweighting schemes compare to the input count matrices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Meeting the GloVe objective [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that GloVe can be thought of as seeking vectors whose dot products are proportional to their PMI values. How close does GloVe come to this in practice? This question asks you to conduct a simple empirical assessment of that: \n",
    "\n",
    "1. Load the matrix stored as `imdb_window5-scaled.csv.gz` in the data distribution. Call this `imdb5`.\n",
    "2. Reweight `imdb5` with Positive PMI.\n",
    "3. Run GloVe on `imdb5` for 10 iterations, learning vectors of dimension 20 (`n=20`). Definitely use the implementation in the `mittens` package, not in `vsm.glove`, else this will take way too long. Except for `max_iter` and `n`, use all the default parameters.\n",
    "4. Report the correlation between the cell values in the PMI and GloVe versions. For this, you can include all 0 values (even though GloVe ignores them). Use `pearsonr` as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Expressive eloooongation [2 points]\n",
    "\n",
    "One of the goals of subword modeling is to capture out-of-vocabulary (OOV) words. This is particularly important for __expressive elogations__ like _coooooool_ and _booriiiing_. Because the amount of elongation is highly variable, we're unlikely to have good representations for such words. How does [our simple approach to subword modeling](vsm_01_distributional.ipynb#Subword-information) do with these phenomena?\n",
    "\n",
    "__Your task:__\n",
    "\n",
    "* Use `vsm.ngram_vsm` to create a 4-gram character-level VSM from the matrix in `imdb_window20-flat.csv.gz`.\n",
    "\n",
    "* Using `character_level_rep` from the notebook for representing words in this space, calculate the cosine distances for pair `cool` and `cooooool`.\n",
    "\n",
    "__To submit__: the cosine distance  between `cool` and `cooooool`\n",
    "\n",
    "(Of course, the broader question we want to answer is whether these words are being modeled as similar, which is a more subjective, comparative question. It does depend on these distance calculations, though.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
